#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Affiliate Product Scraper

Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø² Ù¾Ù„ØªÙØ±Ù…â€ŒÙ‡Ø§ÛŒ Ø§ÙÛŒÙ„ÛŒØª
"""

import logging
import json
import time
import os
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("âŒ Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯: pip install -r requirements.txt")
    exit(1)

# Import platform scrapers
try:
    from platforms.mihanstore import MihanstoreScraper
except ImportError:
    print("âš ï¸ Ù…Ø§Ú˜ÙˆÙ„ platforms Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ø¯Ø± Ù…Ø³ÛŒØ± ØµØ­ÛŒØ­ Ù‡Ø³ØªÛŒØ¯.")
    MihanstoreScraper = None

# Setup Logging
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / 'scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class AffiliateProductScraper:
    """Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª"""
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Args:
            config: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        """
        self.config = config or self._load_config()
        
        # Initialize platform scrapers
        self.scrapers = {}
        
        if MihanstoreScraper:
            mihanstore_config = self.config.get('MIHANSTORE_CONFIG', {})
            if mihanstore_config.get('enabled', True):
                self.scrapers['mihanstore'] = MihanstoreScraper(
                    affiliate_id=mihanstore_config.get('affiliate_id', 'dotshop'),
                    config=mihanstore_config
                )
                logger.info("âœ… Mihanstore scraper loaded")
        
        logger.info(f"âœ… AffiliateProductScraper initialized with {len(self.scrapers)} platform(s)")
    
    def _load_config(self) -> Dict:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª"""
        try:
            import config
            return {
                'MIHANSTORE_CONFIG': config.MIHANSTORE_CONFIG,
                'DIGIKALA_CONFIG': config.DIGIKALA_CONFIG,
                'SCRAPING_CONFIG': config.SCRAPING_CONFIG,
            }
        except ImportError:
            logger.warning("âš ï¸ config.py not found. Using default settings.")
            return {
                'MIHANSTORE_CONFIG': {'enabled': True, 'affiliate_id': 'dotshop'},
                'DIGIKALA_CONFIG': {'enabled': False},
                'SCRAPING_CONFIG': {},
            }
    
    def scrape_mihanstore(self, max_products: int = 30) -> List[Dict]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø² Ù…ÛŒÙ‡Ù† Ø§Ø³ØªÙˆØ±
        
        Args:
            max_products: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„
            
        Returns:
            Ù„ÛŒØ³Øª Ù…Ø­ØµÙˆÙ„Ø§Øª
        """
        if 'mihanstore' not in self.scrapers:
            logger.error("âŒ Mihanstore scraper not available")
            return []
        
        logger.info("ğŸ” Starting Mihanstore scraping...")
        return self.scrapers['mihanstore'].scrape_popular_products(max_products)
    
    def scrape_all_platforms(self) -> Dict[str, List[Dict]]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø² ØªÙ…Ø§Ù… Ù¾Ù„ØªÙØ±Ù…â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„
        
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø´Ø§Ù…Ù„ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù‡Ø± Ù¾Ù„ØªÙØ±Ù…
        """
        logger.info("ğŸš€ Starting scraping from all platforms...")
        results = {}
        
        # Mihanstore
        if 'mihanstore' in self.scrapers:
            try:
                config = self.config.get('MIHANSTORE_CONFIG', {})
                max_products = config.get('max_products', 30)
                results['mihanstore'] = self.scrape_mihanstore(max_products)
            except Exception as e:
                logger.error(f"âŒ Mihanstore error: {e}")
                results['mihanstore'] = []
        
        # TODO: Add other platforms (Digikala, etc.)
        
        total = sum(len(v) for v in results.values())
        logger.info(f"âœ… Scraping completed. Total products: {total}")
        
        return results
    
    def save_to_json(self, data: Dict, filepath: str = 'data/products.json'):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ JSON"""
        # Create data directory if not exists
        Path(filepath).parent.mkdir(exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ Data saved to {filepath}")
    
    def generate_summary(self, data: Dict) -> Dict:
        """ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ"""
        summary = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'platforms': {},
            'total_products': 0,
        }
        
        for platform, products in data.items():
            if products:
                summary['platforms'][platform] = {
                    'count': len(products),
                    'avg_price': sum(p.get('price', 0) for p in products) / len(products),
                    'categories': list(set(p.get('category', 'N/A') for p in products)),
                }
                summary['total_products'] += len(products)
        
        return summary


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    logger.info("="*70)
    logger.info("ğŸš€ Affiliate Automation System - Phase 1: Data Collection")
    logger.info("="*70)
    
    # Ø§ÛŒØ¬Ø§Ø¯ instance Ø§Ø² scraper
    scraper = AffiliateProductScraper()
    
    # Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª
    products = scraper.scrape_all_platforms()
    
    # ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡
    summary = scraper.generate_summary(products)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± JSON
    scraper.save_to_json(products, 'data/products.json')
    scraper.save_to_json(summary, 'data/summary.json')
    
    # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡
    logger.info("\n" + "="*70)
    logger.info("ğŸ“Š SUMMARY")
    logger.info("="*70)
    logger.info(f"Total Products: {summary['total_products']}")
    for platform, stats in summary['platforms'].items():
        logger.info(f"  â€¢ {platform.upper()}: {stats['count']} products")
        logger.info(f"    Average Price: {stats['avg_price']:,.0f} ØªÙˆÙ…Ø§Ù†")
    logger.info("="*70)
    
    logger.info("âœ… Process completed successfully!")


if __name__ == '__main__':
    main()
