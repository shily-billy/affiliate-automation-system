#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Affiliate Product Scraper

Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø² Ù¾Ù„ØªÙØ±Ù…â€ŒÙ‡Ø§ÛŒ Ø§ÙÛŒÙ„ÛŒØª
"""

import logging
import json
import time
import os
import sys
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("âŒ Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯: pip install -r requirements.txt")
    exit(1)

# Import platform scrapers
try:
    from platforms.mihanstore import MihanstoreScraper
except ImportError:
    print("âš ï¸ Ù…Ø§Ú˜ÙˆÙ„ platforms Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ø¯Ø± Ù…Ø³ÛŒØ± ØµØ­ÛŒØ­ Ù‡Ø³ØªÛŒØ¯.")
    MihanstoreScraper = None

# Import Google Sheets (optional)
try:
    from google_sheets import GoogleSheetsManager
    SHEETS_AVAILABLE = True
except ImportError:
    SHEETS_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("âš ï¸ Google Sheets ØºÛŒØ±ÙØ¹Ø§Ù„ - Ù¾Ú©ÛŒØ¬â€ŒÙ‡Ø§ÛŒ Google API Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡")

# Setup Logging
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / 'scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class AffiliateProductScraper:
    """Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª"""
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Args:
            config: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        """
        self.config = config or self._load_config()
        
        # Initialize platform scrapers
        self.scrapers = {}
        
        if MihanstoreScraper:
            mihanstore_config = self.config.get('MIHANSTORE_CONFIG', {})
            if mihanstore_config.get('enabled', True):
                store_url = mihanstore_config.get('store_url', 'https://dot-shop.mihanstore.net')
                self.scrapers['mihanstore'] = MihanstoreScraper(
                    store_url=store_url,
                    config=mihanstore_config
                )
                logger.info(f"âœ… Mihanstore scraper loaded for: {store_url}")
        
        # Initialize Google Sheets (if enabled)
        self.sheets_manager = None
        sheets_config = self.config.get('GOOGLE_SHEETS_CONFIG', {})
        
        if SHEETS_AVAILABLE and sheets_config.get('enabled', False):
            try:
                credentials_file = sheets_config.get('credentials_file', 'credentials.json')
                self.sheets_manager = GoogleSheetsManager(
                    credentials_file=credentials_file,
                    config=sheets_config
                )
                
                # Ø³Ø§Ø®Øª Spreadsheet Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù‡
                if not sheets_config.get('spreadsheet_id'):
                    spreadsheet_id = self.sheets_manager.create_spreadsheet()
                    logger.info(f"âœ… Spreadsheet Ø¬Ø¯ÛŒØ¯ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯")
                    logger.info(f"ðŸ”— URL: {self.sheets_manager.get_spreadsheet_url()}")
                    logger.info(f"âš ï¸  Ù„Ø·ÙØ§Ù‹ ID Ø±Ø§ Ø¯Ø± config.py Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†ÛŒØ¯: {spreadsheet_id}")
                
                logger.info("âœ… Google Sheets Integration ÙØ¹Ø§Ù„")
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Google Sheets: {e}")
                logger.info("Ø±Ø§Ù‡Ù†Ù…Ø§: docs/GOOGLE_SHEETS_SETUP.md")
                self.sheets_manager = None
        
        logger.info(f"âœ… AffiliateProductScraper initialized with {len(self.scrapers)} platform(s)")
    
    def _load_config(self) -> Dict:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª"""
        try:
            import config
            return {
                'MIHANSTORE_CONFIG': config.MIHANSTORE_CONFIG,
                'DIGIKALA_CONFIG': getattr(config, 'DIGIKALA_CONFIG', {'enabled': False}),
                'GOOGLE_SHEETS_CONFIG': getattr(config, 'GOOGLE_SHEETS_CONFIG', {'enabled': False}),
                'SCRAPING_CONFIG': getattr(config, 'SCRAPING_CONFIG', {}),
            }
        except ImportError:
            logger.warning("âš ï¸ config.py not found. Using default settings.")
            return {
                'MIHANSTORE_CONFIG': {
                    'enabled': True, 
                    'store_url': 'https://dot-shop.mihanstore.net',
                    'max_products': 30
                },
                'DIGIKALA_CONFIG': {'enabled': False},
                'GOOGLE_SHEETS_CONFIG': {'enabled': False},
                'SCRAPING_CONFIG': {},
            }
    
    def scrape_mihanstore(self, max_products: int = 30) -> List[Dict]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø² Ù…ÛŒÙ‡Ù† Ø§Ø³ØªÙˆØ±
        
        Args:
            max_products: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„
            
        Returns:
            Ù„ÛŒØ³Øª Ù…Ø­ØµÙˆÙ„Ø§Øª
        """
        if 'mihanstore' not in self.scrapers:
            logger.error("âŒ Mihanstore scraper not available")
            return []
        
        logger.info("ðŸ” Starting Mihanstore scraping...")
        return self.scrapers['mihanstore'].scrape_all_products(max_products)
    
    def scrape_all_platforms(self) -> Dict[str, List[Dict]]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø² ØªÙ…Ø§Ù… Ù¾Ù„ØªÙØ±Ù…â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„
        
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø´Ø§Ù…Ù„ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù‡Ø± Ù¾Ù„ØªÙØ±Ù…
        """
        logger.info("ðŸš€ Starting scraping from all platforms...")
        results = {}
        
        # Mihanstore
        if 'mihanstore' in self.scrapers:
            try:
                config = self.config.get('MIHANSTORE_CONFIG', {})
                max_products = config.get('max_products', 30)
                results['mihanstore'] = self.scrape_mihanstore(max_products)
            except Exception as e:
                logger.error(f"âŒ Mihanstore error: {e}")
                results['mihanstore'] = []
        
        # TODO: Add other platforms (Digikala, etc.)
        
        total = sum(len(v) for v in results.values())
        logger.info(f"âœ… Scraping completed. Total products: {total}")
        
        return results
    
    def save_to_json(self, data: Dict, filepath: str = 'data/products.json'):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ JSON"""
        # Create data directory if not exists
        Path(filepath).parent.mkdir(exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"ðŸ’¾ Data saved to {filepath}")
    
    def save_to_sheets(self, data: Dict) -> bool:
        """
        Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Google Sheets
        
        Args:
            data: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª
            
        Returns:
            True Ø§Ú¯Ø± Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯
        """
        if not self.sheets_manager:
            logger.warning("âš ï¸ Google Sheets ØºÛŒØ±ÙØ¹Ø§Ù„ - ÙÙ‚Ø· Ø¯Ø± JSON Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
            return False
        
        try:
            # ØªØ¨Ø¯ÛŒÙ„ dict Ø¨Ù‡ list
            all_products = []
            for platform, products in data.items():
                all_products.extend(products)
            
            if not all_products:
                logger.warning("âš ï¸ Ù‡ÛŒÚ† Ù…Ø­ØµÙˆÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
                return False
            
            # Ø¢Ù¾Ù„ÙˆØ¯ Ø¨Ù‡ Sheets
            stats = self.sheets_manager.upload_products(all_products, mode='update')
            
            logger.info(f"ðŸ“Š Google Sheets Stats:")
            logger.info(f"   âž• Added: {stats['added']}")
            logger.info(f"   ðŸ”„ Updated: {stats['updated']}")
            logger.info(f"   ðŸŸ¢ Unchanged: {stats['unchanged']}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ù„ÙˆØ¯ Google Sheets: {e}")
            return False
    
    def generate_summary(self, data: Dict) -> Dict:
        """ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ"""
        summary = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'platforms': {},
            'total_products': 0,
        }
        
        for platform, products in data.items():
            if products:
                prices = [p.get('price', 0) for p in products if p.get('price', 0) > 0]
                summary['platforms'][platform] = {
                    'count': len(products),
                    'avg_price': sum(prices) / len(prices) if prices else 0,
                    'min_price': min(prices) if prices else 0,
                    'max_price': max(prices) if prices else 0,
                }
                summary['total_products'] += len(products)
        
        return summary


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    logger.info("="*70)
    logger.info("ðŸš€ Affiliate Automation System - Phase 1: Data Collection")
    logger.info("="*70)
    
    # Ø§ÛŒØ¬Ø§Ø¯ instance Ø§Ø² scraper
    scraper = AffiliateProductScraper()
    
    # Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª
    products = scraper.scrape_all_platforms()
    
    # ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡
    summary = scraper.generate_summary(products)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± JSON
    scraper.save_to_json(products, 'data/products.json')
    scraper.save_to_json(summary, 'data/summary.json')
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Google Sheets (Ø§Ú¯Ø± ÙØ¹Ø§Ù„ Ø¨Ø§Ø´Ù‡)
    scraper.save_to_sheets(products)
    
    # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡
    logger.info("\n" + "="*70)
    logger.info("ðŸ“Š SUMMARY")
    logger.info("="*70)
    logger.info(f"Total Products: {summary['total_products']}")
    for platform, stats in summary['platforms'].items():
        logger.info(f"  â€¢ {platform.upper()}: {stats['count']} products")
        if stats['avg_price'] > 0:
            logger.info(f"    Average Price: {stats['avg_price']:,.0f} ØªÙˆÙ…Ø§Ù†")
            logger.info(f"    Price Range: {stats['min_price']:,.0f} - {stats['max_price']:,.0f} ØªÙˆÙ…Ø§Ù†")
    
    # Ù†Ù…Ø§ÛŒØ´ Ù„ÛŒÙ†Ú© Google Sheets
    if scraper.sheets_manager:
        url = scraper.sheets_manager.get_spreadsheet_url()
        if url:
            logger.info(f"\nðŸ”— Google Sheets: {url}")
    
    logger.info("="*70)
    
    logger.info("âœ… Process completed successfully!")


if __name__ == '__main__':
    main()
