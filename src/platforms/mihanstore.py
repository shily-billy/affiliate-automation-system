#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Mihanstore Scraper Module

Ù…Ø§Ú˜ÙˆÙ„ Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø² Ù…ÛŒÙ‡Ù† Ø§Ø³ØªÙˆØ±
"""

import logging
import time
import re
from typing import List, Dict, Optional
from urllib.parse import urljoin

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    raise ImportError("Ù„Ø·ÙØ§Ù‹ requirements.txt Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯")

logger = logging.getLogger(__name__)


class MihanstoreScraper:
    """Ú©Ù„Ø§Ø³ scraper Ø¨Ø±Ø§ÛŒ Ù…ÛŒÙ‡Ù† Ø§Ø³ØªÙˆØ±"""
    
    BASE_URL = "https://mihanstore.net"
    AFFILIATE_URL = "https://affiliate-marketing.mihanstore.net"
    
    def __init__(self, affiliate_id: str = None, config: Optional[Dict] = None):
        """
        Args:
            affiliate_id: Ø´Ù†Ø§Ø³Ù‡ Ø§ÙÛŒÙ„ÛŒØª Ø´Ù…Ø§
            config: ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø¶Ø§ÙÛŒ
        """
        self.affiliate_id = affiliate_id or "dotshop"
        self.config = config or {}
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fa,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
        
        logger.info(f"âœ… MihanstoreScraper initialized with affiliate_id: {self.affiliate_id}")
    
    def _clean_price(self, price_text: str) -> int:
        """
        Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ù‚ÛŒÙ…Øª Ø¨Ù‡ Ø¹Ø¯Ø¯
        
        Args:
            price_text: Ù…ØªÙ† Ù‚ÛŒÙ…Øª (Ù…Ø«Ù„: "248,000 ØªÙˆÙ…Ø§Ù†")
            
        Returns:
            Ù‚ÛŒÙ…Øª Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ø¯Ø¯ ØµØ­ÛŒØ­ (ØªÙˆÙ…Ø§Ù†)
        """
        if not price_text:
            return 0
        
        # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ø¹Ø¯Ø¯ÛŒ
        numbers = re.sub(r'[^0-9]', '', price_text)
        
        try:
            return int(numbers) if numbers else 0
        except ValueError:
            logger.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ù‚ÛŒÙ…Øª: {price_text}")
            return 0
    
    def _build_affiliate_link(self, product_url: str) -> str:
        """
        Ø³Ø§Ø®Øª Ù„ÛŒÙ†Ú© Ø§ÙÛŒÙ„ÛŒØª
        
        Args:
            product_url: Ù„ÛŒÙ†Ú© Ø§ØµÙ„ÛŒ Ù…Ø­ØµÙˆÙ„
            
        Returns:
            Ù„ÛŒÙ†Ú© Ø§ÙÛŒÙ„ÛŒØª Ú©Ø§Ù…Ù„
        """
        if '?' in product_url:
            return f"{product_url}&ref={self.affiliate_id}"
        else:
            return f"{product_url}?ref={self.affiliate_id}"
    
    def scrape_category(self, category_url: str, max_products: int = 50) -> List[Dict]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø² ÛŒÚ© Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
        
        Args:
            category_url: Ù„ÛŒÙ†Ú© Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
            max_products: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„
            
        Returns:
            Ù„ÛŒØ³Øª Ù…Ø­ØµÙˆÙ„Ø§Øª
        """
        logger.info(f"ğŸ” Scraping category: {category_url}")
        products = []
        
        try:
            # Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ ØµÙØ­Ù‡ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
            response = self.session.get(category_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'lxml')
            
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¹Ù†Ø§ØµØ± Ù…Ø­ØµÙˆÙ„
            # Ù…ÛŒÙ‡Ù† Ø§Ø³ØªÙˆØ± Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± product-card Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
            product_cards = soup.select('.product-card, .product-item, .product, article.product')
            
            if not product_cards:
                logger.warning("âš ï¸ Ù…Ø­ØµÙˆÙ„ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ø³Ø§Ø®ØªØ§Ø± HTML ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡.")
                # ØªÙ„Ø§Ø´ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø¯ÛŒÚ¯Ø±
                product_cards = soup.find_all('div', class_=re.compile(r'product', re.I))
            
            logger.info(f"âœ… Found {len(product_cards)} products")
            
            for idx, card in enumerate(product_cards[:max_products], 1):
                try:
                    product = self._extract_product_info(card)
                    if product:
                        products.append(product)
                        logger.info(f"  [{idx}/{min(max_products, len(product_cards))}] {product['name'][:50]}...")
                    
                    # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
                    time.sleep(0.5)
                    
                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØµÙˆÙ„ {idx}: {e}")
                    continue
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø´Ø¨Ú©Ù‡: {e}")
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡: {e}")
        
        logger.info(f"âœ… Scraped {len(products)} products from category")
        return products
    
    def _extract_product_info(self, card) -> Optional[Dict]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„ Ø§Ø² Ú©Ø§Ø±Øª
        
        Args:
            card: Ø¹Ù†ØµØ± BeautifulSoup Ú©Ø§Ø±Øª Ù…Ø­ØµÙˆÙ„
            
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„
        """
        try:
            # Ù†Ø§Ù… Ù…Ø­ØµÙˆÙ„
            name_elem = card.select_one('h3, h2, .product-title, .title, a[title]')
            name = name_elem.get_text(strip=True) if name_elem else None
            if not name and name_elem:
                name = name_elem.get('title', '').strip()
            
            if not name:
                return None
            
            # Ù„ÛŒÙ†Ú© Ù…Ø­ØµÙˆÙ„
            link_elem = card.select_one('a[href]')
            link = link_elem.get('href', '') if link_elem else ''
            
            if link and not link.startswith('http'):
                link = urljoin(self.BASE_URL, link)
            
            # Ù‚ÛŒÙ…Øª
            price_elem = card.select_one('.price, .product-price, .price-current, span[class*="price"]')
            price_text = price_elem.get_text(strip=True) if price_elem else '0'
            price = self._clean_price(price_text)
            
            # ØªØµÙˆÛŒØ±
            img_elem = card.select_one('img')
            image = ''
            if img_elem:
                image = img_elem.get('src') or img_elem.get('data-src') or img_elem.get('data-lazy-src') or ''
                if image and not image.startswith('http'):
                    image = urljoin(self.BASE_URL, image)
            
            # Ø³Ø§Ø®Øª Ù„ÛŒÙ†Ú© Ø§ÙÛŒÙ„ÛŒØª
            affiliate_link = self._build_affiliate_link(link) if link else ''
            
            # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
            category_elem = card.select_one('.category, .product-category')
            category = category_elem.get_text(strip=True) if category_elem else 'Fashion'
            
            product_data = {
                'name': name,
                'price': price,
                'price_formatted': f"{price:,} ØªÙˆÙ…Ø§Ù†",
                'image': image,
                'link': link,
                'affiliate_link': affiliate_link,
                'category': category,
                'platform': 'mihanstore',
                'commission_rate': 10,  # Ù†Ø±Ø® Ú©Ù…ÛŒØ³ÛŒÙˆÙ† Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 10%
                'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            }
            
            return product_data
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª: {e}")
            return None
    
    def scrape_popular_products(self, max_products: int = 30) -> List[Dict]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª Ù…Ø­Ø¨ÙˆØ¨/Ù¾Ø±ÙØ±ÙˆØ´
        
        Args:
            max_products: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„
            
        Returns:
            Ù„ÛŒØ³Øª Ù…Ø­ØµÙˆÙ„Ø§Øª
        """
        logger.info("ğŸ”¥ Scraping popular products...")
        return self.scrape_category(self.BASE_URL, max_products)
    
    def scrape_by_categories(self, categories: List[str], max_per_category: int = 20) -> Dict[str, List[Dict]]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§
        
        Args:
            categories: Ù„ÛŒØ³Øª Ù„ÛŒÙ†Ú© Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§
            max_per_category: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„ Ù‡Ø± Ø¯Ø³ØªÙ‡
            
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø´Ø§Ù…Ù„ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù‡Ø± Ø¯Ø³ØªÙ‡
        """
        results = {}
        
        for category_url in categories:
            try:
                category_name = category_url.split('/')[-1] or 'main'
                logger.info(f"\nğŸ“‚ Processing category: {category_name}")
                
                products = self.scrape_category(category_url, max_per_category)
                results[category_name] = products
                
                # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡ {category_url}: {e}")
                results[category_url] = []
        
        total = sum(len(prods) for prods in results.values())
        logger.info(f"\nâœ… Total products scraped: {total}")
        
        return results


if __name__ == '__main__':
    # ØªØ³Øª Ø³Ø±ÛŒØ¹
    logging.basicConfig(level=logging.INFO)
    
    scraper = MihanstoreScraper(affiliate_id='dotshop')
    products = scraper.scrape_popular_products(max_products=10)
    
    print(f"\n\nğŸ“¦ Total: {len(products)} products")
    if products:
        print("\nğŸ‘‡ Sample product:")
        import json
        print(json.dumps(products[0], ensure_ascii=False, indent=2))
