#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Mihanstore Storefront Scraper

Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø² ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù…ÛŒÙ‡Ù† Ø§Ø³ØªÙˆØ±
Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø± ÙˆØ§Ù‚Ø¹ÛŒ: product.php?id=XXXX
"""

import logging
import time
import re
from typing import List, Dict, Optional, Set
from urllib.parse import urljoin, urlparse, parse_qs

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    raise ImportError("Ù„Ø·ÙØ§Ù‹ requirements.txt Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯")

logger = logging.getLogger(__name__)


class MihanstoreScraper:
    """Ø§Ø³Ú©Ø±ÛŒÙ¾Ø± ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù…ÛŒÙ‡Ù† Ø§Ø³ØªÙˆØ±"""
    
    def __init__(self, store_url: str = "https://dot-shop.mihanstore.net", config: Optional[Dict] = None):
        """
        Args:
            store_url: Ø¢Ø¯Ø±Ø³ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø´Ù…Ø§ Ø¯Ø± Ù…ÛŒÙ‡Ù† Ø§Ø³ØªÙˆØ±
            config: ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø¶Ø§ÙÛŒ
        """
        self.store_url = store_url.rstrip('/')
        self.config = config or {}
        
        # Fallback domains Ø§Ú¯Ø± Ø¯Ø³ØªØ±Ø³ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù†Ø¯Ø§Ø´ØªÛŒÙ…
        self.fallback_domains = [
            "https://mihanstore.net",
            "https://www3.mihanstore.net",
        ]
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fa,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
        
        logger.info(f"âœ… MihanstoreScraper initialized for: {self.store_url}")
    
    def _clean_price(self, price_text: str) -> int:
        """
        ØªØ¨Ø¯ÛŒÙ„ Ù‚ÛŒÙ…Øª Ø¨Ù‡ Ø¹Ø¯Ø¯
        Ù…Ø«Ø§Ù„: "1,698,000 ØªÙˆÙ…Ø§Ù†" -> 1698000
        
        Args:
            price_text: Ù…ØªÙ† Ù‚ÛŒÙ…Øª
            
        Returns:
            Ù‚ÛŒÙ…Øª Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ø¯Ø¯ ØµØ­ÛŒØ­ (ØªÙˆÙ…Ø§Ù†)
        """
        if not price_text:
            return 0
        
        # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ø¹Ø¯Ø¯ÛŒ (Ø¬Ø² Ù…Ù…ÛŒØ² Ùˆ Ù†Ù‚Ø·Ù‡)
        numbers = re.sub(r'[^0-9]', '', price_text)
        
        try:
            return int(numbers) if numbers else 0
        except ValueError:
            logger.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ù‚ÛŒÙ…Øª: {price_text}")
            return 0
    
    def _fetch_page(self, url: str, use_fallback: bool = True) -> Optional[BeautifulSoup]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ùˆ parse Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡
        
        Args:
            url: Ø¢Ø¯Ø±Ø³ ØµÙØ­Ù‡
            use_fallback: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
            
        Returns:
            BeautifulSoup object ÛŒØ§ None
        """
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'lxml')
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª {url}: {e}")
            
            # ØªÙ„Ø§Ø´ Ø¨Ø§ fallback domains
            if use_fallback and 'product.php' in url:
                product_id = self._extract_product_id(url)
                if product_id:
                    for fallback_domain in self.fallback_domains:
                        try:
                            fallback_url = f"{fallback_domain}/product.php?id={product_id}"
                            logger.info(f"ğŸ”„ ØªÙ„Ø§Ø´ Ø¨Ø§: {fallback_url}")
                            response = self.session.get(fallback_url, timeout=30)
                            response.raise_for_status()
                            return BeautifulSoup(response.content, 'lxml')
                        except:
                            continue
            
            return None
    
    def _extract_product_id(self, url: str) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ID Ù…Ø­ØµÙˆÙ„ Ø§Ø² URL"""
        try:
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            return params.get('id', [None])[0]
        except:
            return None
    
    def discover_product_links(self, max_products: int = 50) -> Set[str]:
        """
        Ú©Ø´Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø² ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡
        
        Args:
            max_products: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„
            
        Returns:
            Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª
        """
        logger.info(f"ğŸ” Ø´Ø±ÙˆØ¹ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§Ø²: {self.store_url}")
        
        product_links = set()
        
        # Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ
        soup = self._fetch_page(self.store_url)
        if not soup:
            logger.error("âŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù…Ù…Ú©Ù† Ù†ÛŒØ³Øª")
            return product_links
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ product.php?id=
        for link in soup.find_all('a', href=True):
            href = link['href']
            
            # Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ product.php?id= Ø¯Ø§Ø±Ù‡
            if 'product.php' in href and 'id=' in href:
                # Ø³Ø§Ø®Øª URL Ú©Ø§Ù…Ù„
                full_url = urljoin(self.store_url, href)
                product_links.add(full_url)
                
                if len(product_links) >= max_products:
                    break
        
        logger.info(f"âœ… {len(product_links)} Ù…Ø­ØµÙˆÙ„ Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
        return product_links
    
    def scrape_product(self, product_url: str) -> Optional[Dict]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÛŒÚ© Ù…Ø­ØµÙˆÙ„
        
        Args:
            product_url: Ù„ÛŒÙ†Ú© Ù…Ø­ØµÙˆÙ„
            
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„
        """
        product_id = self._extract_product_id(product_url)
        if not product_id:
            logger.warning(f"âš ï¸ ID Ù…Ø­ØµÙˆÙ„ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {product_url}")
            return None
        
        logger.debug(f"ğŸ” Ø¯Ø± Ø­Ø§Ù„ scraping Ù…Ø­ØµÙˆÙ„ ID: {product_id}")
        
        # Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡ Ù…Ø­ØµÙˆÙ„
        soup = self._fetch_page(product_url, use_fallback=True)
        if not soup:
            logger.warning(f"âš ï¸ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…Ø­ØµÙˆÙ„ {product_id} Ù…Ù…Ú©Ù† Ù†ÛŒØ³Øª")
            return None
        
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ù…Ø­ØµÙˆÙ„
            name = None
            # ØªÙ„Ø§Ø´ 1: Ø§Ø² title ØµÙØ­Ù‡
            if soup.title:
                name = soup.title.get_text(strip=True)
                # Ø­Ø°Ù "Ù…ÛŒÙ‡Ù† Ø§Ø³ØªÙˆØ±" ÛŒØ§ Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ø¶Ø§ÙÛŒ Ø§Ø² Ø¢Ø®Ø±
                name = re.sub(r'\s*[-|]\s*(Ù…ÛŒÙ‡Ù† Ø§Ø³ØªÙˆØ±|Ø®Ø±ÛŒØ¯ Ù¾Ø³ØªÛŒ).*$', '', name, flags=re.IGNORECASE)
            
            # ØªÙ„Ø§Ø´ 2: Ø§Ø² h1
            if not name:
                h1 = soup.find('h1')
                if h1:
                    name = h1.get_text(strip=True)
            
            # ØªÙ„Ø§Ø´ 3: Ø§Ø² Ù‡Ø± Ø¹Ù†ØµØ± Ø¨Ø§ class Ø­Ø§ÙˆÛŒ "product" Ùˆ "title"
            if not name:
                title_elem = soup.find(class_=re.compile(r'product.*title|title.*product', re.I))
                if title_elem:
                    name = title_elem.get_text(strip=True)
            
            if not name:
                logger.warning(f"âš ï¸ Ù†Ø§Ù… Ù…Ø­ØµÙˆÙ„ {product_id} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯")
                return None
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÛŒÙ…Øª
            price = 0
            # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆÛŒ Ù‚ÛŒÙ…Øª: Ø¹Ø¯Ø¯ + Ú©Ø§Ù…Ø§ + "ØªÙˆÙ…Ø§Ù†"
            price_pattern = r'([0-9,]+)\s*ØªÙˆÙ…Ø§Ù†'
            price_matches = soup.find_all(text=re.compile(price_pattern))
            
            if price_matches:
                # Ú¯Ø±ÙØªÙ† Ø§ÙˆÙ„ÛŒÙ† Ù‚ÛŒÙ…Øª Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡
                price_text = str(price_matches[0])
                price = self._clean_price(price_text)
            else:
                # ØªÙ„Ø§Ø´ Ø¨Ø§ Ø³Ù„Ú©ØªÙˆØ±Ù‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„
                price_selectors = ['.price', '.product-price', '[class*="price"]', 'span.price']
                for selector in price_selectors:
                    price_elem = soup.select_one(selector)
                    if price_elem:
                        price = self._clean_price(price_elem.get_text())
                        if price > 0:
                            break
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ù…Ø­ØµÙˆÙ„
            image_url = None
            
            # ØªÙ„Ø§Ø´ 1: ØªØµÙˆÛŒØ± Ø¨Ø§ id ÛŒØ§ class Ø®Ø§Øµ Ù…Ø­ØµÙˆÙ„
            img_elem = soup.select_one('img[class*="product"], img[id*="product"], .product-image img')
            if img_elem:
                image_url = img_elem.get('src') or img_elem.get('data-src')
            
            # ØªÙ„Ø§Ø´ 2: Ø§ÙˆÙ„ÛŒÙ† ØªØµÙˆÛŒØ± Ø¨Ø²Ø±Ú¯ Ø¯Ø± Ù…Ø­ØªÙˆØ§
            if not image_url:
                for img in soup.find_all('img'):
                    src = img.get('src') or img.get('data-src')
                    if src and not any(x in src.lower() for x in ['logo', 'icon', 'banner', 'button']):
                        image_url = src
                        break
            
            if image_url and not image_url.startswith('http'):
                image_url = urljoin(self.store_url, image_url)
            
            # Ø³Ø§Ø®Øª Ù„ÛŒÙ†Ú© Ù…Ø­ØµÙˆÙ„ Ø±ÙˆÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø®ÙˆØ¯ØªÙˆÙ†
            product_link = f"{self.store_url}/product.php?id={product_id}"
            
            product_data = {
                'product_id': product_id,
                'name': name.strip(),
                'price': price,
                'price_formatted': f"{price:,} ØªÙˆÙ…Ø§Ù†" if price > 0 else "ØªÙ…Ø§Ø³ Ø¨Ú¯ÛŒØ±ÛŒØ¯",
                'image': image_url or '',
                'product_url': product_link,
                'platform': 'mihanstore',
                'store': self.store_url,
                'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            }
            
            logger.debug(f"âœ… Ù…Ø­ØµÙˆÙ„ {product_id}: {name[:50]}... - {price:,} ØªÙˆÙ…Ø§Ù†")
            return product_data
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØµÙˆÙ„ {product_id}: {e}")
            return None
    
    def scrape_all_products(self, max_products: int = 30) -> List[Dict]:
        """
        Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… Ù…Ø­ØµÙˆÙ„Ø§Øª ÙØ±ÙˆØ´Ú¯Ø§Ù‡
        
        Args:
            max_products: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„
            
        Returns:
            Ù„ÛŒØ³Øª Ù…Ø­ØµÙˆÙ„Ø§Øª
        """
        logger.info(f"ğŸš€ Ø´Ø±ÙˆØ¹ scraping ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {self.store_url}")
        
        # Ú©Ø´Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª
        product_links = self.discover_product_links(max_products)
        
        if not product_links:
            logger.warning("âš ï¸ Ù‡ÛŒÚ† Ù…Ø­ØµÙˆÙ„ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")
            return []
        
        # Scrape Ú©Ø±Ø¯Ù† Ù‡Ø± Ù…Ø­ØµÙˆÙ„
        products = []
        total = len(product_links)
        
        for idx, link in enumerate(product_links, 1):
            logger.info(f"[{idx}/{total}] Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´...")
            
            product = self.scrape_product(link)
            if product:
                products.append(product)
            
            # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
            if idx < total:
                time.sleep(1)
        
        logger.info(f"\nâœ… ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(products)}")
        return products


if __name__ == '__main__':
    # ØªØ³Øª Ø³Ø±ÛŒØ¹
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # ØªØ³Øª Ø¨Ø§ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ dot-shop
    scraper = MihanstoreScraper(store_url="https://dot-shop.mihanstore.net")
    products = scraper.scrape_all_products(max_products=10)
    
    print(f"\n\nğŸ“¦ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„: {len(products)} Ù…Ø­ØµÙˆÙ„")
    if products:
        print("\nğŸ‘‡ Ù†Ù…ÙˆÙ†Ù‡ Ù…Ø­ØµÙˆÙ„ Ø§ÙˆÙ„:")
        import json
        print(json.dumps(products[0], ensure_ascii=False, indent=2))
